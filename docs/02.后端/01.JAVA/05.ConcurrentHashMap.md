---
title: ConcurrentHashMap基础文档
date: 2020-08-07 12:20:10
permalink: /pages/java-concurrenHashMap
categories: 
  - 后端
  - java
tags: 
  - concurrenthashmap
author: 
  name: Coder
  link: https://github.com/zyb-github
---

# ConcurrentHashMap

## 1、基本结构

![img](https://s2.loli.net/2022/08/07/FanWhEZQA9eVMKw.jpg)

### 1、和HashMap的相同之处

> 数组、链表的结构几乎相同，所以对底层数据结构的操作思路是相同的（实现不一样）

### 2、和HashMap不同之处

> 红黑树结构不同：HashMap的红黑树节点叫做TreeNode，TreeNode不仅有属性，还维护红黑树的结构，比如说查找、新增等等。
>
> concurrentHashMap中的红黑树被拆成了两块，TreeNode仅仅维护属性和查找功能，新增了TreeBin，来维护红黑树的结构，并负责根节点的加锁和解锁
>
> 新增了ForwardingNode (转移节点)，扩容的时候使用，通过使用该节点，来保证扩容时的线程安全

## 2、基本构成

### 1、重要属性

```java
//这个Node数组就是ConcurrentHashMap用来存储数据的哈希表。
transient volatile Node[] table
//这是默认的初始化哈希表数组大小
private static final int DEFAULT_CAPACITY = 16;
//转化为红黑树的链表长度阈值
static final int TREEIFY_THRESHOLD = 8
//这个标识位用于识别扩容时正在转移数据
static final int MOVED = -1
//计算哈希值时用到的参数，用来去除符号位
static final int HASH_BITS = 0x7fffffff;
//数据转移时，新的哈希表数组
private transient volatile Node[] nextTable;
```

### 2、重要组成元素

+ Node

  > 链表中的元素为node对象，它是链表上的一个节点，内部存储了key、value值，以及下一个节点的引用，这样的一系列的node组成一个链表

+ ForwardingNode

  > 当进行扩容时，要把链表迁移到新的hash表，在做这个操作时，会把数组中的头节点替换为ForwardingNode对象，ForwardingNode对象不保存key和value，只保存了扩容后的哈希表的引用（nextTable），此时查找相应的node，需要去nextTable中查找

+ TreeBin

  > 当链表转为红黑树后，数组中保存的引用为TreeBin，TreeBin内部不保存key、value，它保存了TreeNode的list以及红黑树root

+ TreeNode

  > 红黑树的节点

### 3、核心方法

#### 1、put

```java
public V put(K key, V value){
  return putVal(key, value, false);
}

final V putVal(K key, V value, boolean onlyIfAbsent){
  //ConcurrentHashMap的key和value都不能为null
  if(key==null || value == null) throw new NullPointerException;
  //计算key的hash值
  int hash = spread(key.hashCode());
  //
  int binCount = 0;
  
  for(Node<K,V>[] tab = table;;){
    Node<K,V> f; int n,i,fh;
    if(tab == null || (n = tab.length) == 0){
      tab = initTable();
    }else if ((f = tabAt(tab,i=(n-1) & hash)) == null){
      if(casTabAt(tab,i,null,new Node<K,V>(hash,key,value,null))){
        //no lock when adding to empty bin
        break;
      }
    }else if((fh=f.hash) == MOVED	){
      tab = helpTransfer(tab,f);
    }else{
      V oldVal = null;
      //加锁
      synchronized(f){
        if(tabAt(tab,i) == f){
          if(fh >= 0){
            binCount = 1;
            for(Node<K,V> e = f;;++binCount){
							K ek;
              if(e.hash == hash && (ek != null && k.equals(ek)){
                oldVal = e.val;
                if(!onlyIfAbsent) e.val = value;
                break;
              }
              Node<K,V> pred = e;
              if((e = e.next) == null){
                pred.next = new Node<K,V>(hash,key,value,null);
                break;
              }
            }
          }
          else if(f instanceof TreeBin){
            Node<K,V> p;
            binCount = 2;
            if((p = ((TreeBin<K,V>)f).putTreeVal(hash,key,value) != null){
              oldVal = p.val;
              if(!onlyIfAbsent){
                p.val = value;
              }
            }
          }
        }
        if(binCount != 0){
          if(binCount >= TREETFY_THRESHOLD){
            treeifyBin(tab,i);
          }
          if(oldVal != null){
            return oldVal;
          }
          break;
        }
      }
    }
    addCount(1L,binCount);
    return null;           
  }
}
```

> ConcurrentHashMap在put方法上的整体思路和HashMap相同，但在线程安全方面写了很多保障的代码
>
> 如果数组为空-->初始化--->初始化完成----计算当前槽点有没有值----> 如果没有值，通过cas创建，失败继续自旋（for死循环）直到成功，-----槽点有值的话---> 如果槽点是转移节点，（正在扩容）就会一直自旋等待扩容完成之后再新增，如果不是转移节点-----槽点有值的，先锁定当前节点，保证其余线程不能操作，如果是链表，新增值到链表的尾部，如果是红黑树，使用红黑树新增的方法进行新增-----新增完成之后 检查是否需要扩容，需要的话去扩容

#### 2、put如何保证线程安全

+ 数组初始化的线程安全

  > 数组初始化时，首选通过自旋来保证一定可以初始化成功，然后通过CAS设置SIZECTL的值，来保证同一时刻只能有一个线程来对数组进行初始化，CAS成功之后，还会再次判断当前数组是否已经初始化，如果已经初始化完成，就不会再次进行初始化，通过CAS+自旋+两次Check保证了初始化的线程安全
  >
  > sizectl值的含义
  >
  > -1 代表有线程正在创建table
  >
  > -N 代表有N-1个线程正在复制table
  >
  > 在table初始化前，代表根据构造函数传入的值计算应被初始化的大小
  >
  > 在table初始化后，则被设置为table的75%，代表table的容量

+ 新增槽点值时的线程安全

  > 1、通过自旋死循环保证一定可以新增成功
  >
  > ​	在新增之前，通过 for(Node<K,V>[] tab = table ; ; )这样的死循环来保证新增一定可以成功，一旦新增成功，就可以推出当前死循环，新增失败的话，可以重复新增的步骤，直到新增成功为止。
  >
  > 2、当前槽点为空时，通过CAS新增
  >
  > ​	这里的写法非常严谨，没有在判断槽点为空的情况下直接赋值，因为在判断槽点为空和赋值的瞬间，很有可能这个槽点已经被其他线程赋值了，这里使用CAS算法，能够保证槽点为空的情况下赋值成功，如果恰好槽点被其他线程赋值，当前CAS操作失败，会再次执行for循环，再走一次槽点有值的put流程，这就是自旋+CAS的组合
  >
  > 3、当前槽点有值时，锁住当前槽点
  >
  > ​	put时，如果当前槽点有值，就是key‘的hash冲突情况，此时槽点上可能是链表或者红黑树，通过锁住槽点来保证同一时刻只能有一个线程对槽点进行修改
  >
  > 4、红黑树旋转时，锁住红黑树的根节点，保证同一时刻，当前红黑树只能被一个线程旋转

  + hash算法

    > spread方法源码
    >
    > ```java
    > static final int spread(int h){
    >   return (h ^ (h >>> 16)) & HASH_BITS;
    > }
    > ```
    >
    > hash算法的逻辑，决定concurrentHashMap保存和读取速度
    >
    > 传入的参数h为key的hashcocde，spread对hashcode重新进行的加工，重新计算hash值
    >
    > hash值用来映射该key在哈希表中的位置，取出哈希表中该hash值对应位置的代码如下：
    >
    > ```java
    > tabAt(tab,i=(n-1)&hash);
    > 第一个参数为哈希表，第二个参数是哈希表中的数组下标，通过（n-1）&hash计算下标，n为数组长度，我们以默认大小16为例，那么n-1=15，我们假设hash为100，
    > 那么15转为二进制 0000 0000 0000 0000 0000 0000 0000 1111
    > 100转为二进制0000 0000 0000 0000 0000 0000 0110 0100
    > 计算结果：0000 0000 0000 0000 0000 0000 0000 0100  
    > 结果对应的十进制为 4 
    >   
    > 15的二进制高位都是0，低位都是1，那么经过&运算后，hash值100的高位全部被清零，低位保持不变，并且一定是小于n-1的，也就是说通过这个方法计算出的数组下标，肯定不会越界。
    > ```
    >
    > 提问：
    >
    > 1、数组大小可以是17、18吗
    > 	不可以，数组的大小必须是2的n次方，也就是16、32等，因为如果不是2的n次方，那么经过计算的数组下标会增大碰撞的概率。如果hash的二进制是10000（十进制16）、10010（十进制18）、10001（十进制17），和10010做&计算后，都是10000，也就是都被映射到数组16这个下标上，这三个值会以链表的形式存储在数组16下标的位置，这显然不是我们想要的结果。但如果数组长度是2的n次方，2进制的值是10、100、1000， 作为下标来说需要-1使用，则n-1之后的二进制为 11、111、1111  这样的二进制数和hash值低位&后，会保留原来哈希值的低位数值，那么只要hash值的低位不一样，就不会发生碰撞
    >
    > 2、如果为了保证不越界，那么为什么不使用%取余进行计算？
    >
    > ​	因为位运算的效率更高
    >
    > 3、为什么不直接使用key的hashCode，而是进一步加工hash值？
    >
    > ​	主要是减少碰撞的效率，spread使用的代码是 h ^ (h >>> 16)
    >
    > 这个意思是把h的二进制数据向右移动16位，我们知道整型是32位，那么右移16位，就是把高16位移到了低16位，而高16位进行清零
    >
    > ^ 为异或操作，二进制按位比较，相同为0，不同为1.这行代码的意思就是把高低16位做异或，如果两个hashcode的低16 相同，但是高位不同，经过此运算，低16位也会变得不同。
    >
    > 那为什么要这么做呢： 这是因为哈希表数组长度n会是偏小的数值，那么进行（n-1）&hash运算时，一直使用的是hash较低位的值，那么即使hash值不同，但如果低位相当，也会发生碰撞，而进行了h^(h>>>16)加工后的hash值，让hashCode的高位也加入了哈希运算，因此减少了碰撞概率
    >
    > h ^ ( h >>> 16) & HASH_BITS
    >
    > 为什么最后还要和常量进行&计算呢？
    >
    > HASH_BITS常量值位0✖️7fffffff，转化为2进制为：0111 1111 1111 1111 1111 1111 1111 1111
    >
    > 这个操作会把最高位转为0，其实为了消除符号位，得到的都是正数，这是因为负的hashCode在concrrentHashMap中有特殊的含义，因此需要得到一个正的hashCode

#### 3、get

> concurrentHashMap读取比较简单：先获取数组下标，然后通过判断数组下标的key‘是否和我们的相等，相等的话直接返回，如果下标的槽点是链表或者红黑树，分别调用相应的查找数据的方法，整体思路和hashmap相似

```java
public V get(Object key){
  Node<K,V>[] tab; Node<K,V> e,p;int n,eh;K ek;
  int h = spread(key.hashCode());
  if((tab = table) != null && (n = tab.length) > 0 && 
    (e= tabAt(tab,(n-1) & h)) != null){
    if((eh = e.hash) == h){
      if((ek = e.key) == key || (ek != null && key.equals(ek))){
        return e.val;
      }
    }
    else if(eh < 0){
      return (p = e.find(h,key)) != null ? p.val : null;
    }
    while((e = e.next) != null){
      if(e.hash == h && ((ek = e.key) == key  || (ek != null && key.equals(ek)))){
        return e.val;
      }
    }
  }
  return null;
} 
```



### 4、扩容源码

#### 1、为什么需要扩容？

> 当数组中保存的链表越来越多，那么再存储进来的元素大概率会插入到现有的链表中，而不是使用数组中剩余的空位，这样会造成数组中保存的链表越来越长，由此导致哈希表查找速度下降，从O(1)趋近于 链表 的.  O(n/2). 所以concrrentHashMap会做一个扩容操作，也就是把数组长度变大，增加更多的空位出来，最终目的是为了防止链表过长，这样查找的时间复杂度才会去趋近于O(1).

#### 2、什么时候扩容？

> 扩容的操作不是在数组没有空位时进行，因为在空位快满时，新保存的元素更大的概率会命中已经存在的位置，那么可能最后几个桶位很难被使用，而链表却越来越长，另外concurrentHashMap还会有链表转为红黑树的过程，以提高查找的速度，红黑树的时间复杂度位O(logn),链表的时间复杂度为O(n/2),因此只有在O(logn) < O(n/2)时才会进行转换，也就是8作为分界点

```java
//treeifyBin 代码，会选择把此时的保存的数据所在的链表转为红黑树，还是对整个哈希表进行扩容
private final void treeifyBin(Node<K,V>[] tab, int index){
  Node<K,V> b; int n, sc;
  if(tab != null){
    //如果哈希表的长度小于64，那么选择扩大哈希表的大小，而不是把链表转为红黑树
    if((n = tab.length) < MIN_TREEIFY_CAPACTIY){
      tryPresize(n << 1);
    }else if((b = tabAt(tab,index) != null %% b.hash >= 0)){
      //将哈希表中的index位置的链表转为红黑树
      synchronized(b){
        //下面是将Node链表转为TreeNode链表
        if((tabAt(tab,index) == b)){
          TreeNode<K,V> hd = null, tl=null;
          for(Node<K,V> e=b; e != null;e=e.next){
            TreeNode<K,V> p = new TreeNode<K,V>(e.hash,e.key,e.val,null,null);
            if((p.prev = tl) == null){
              hd = p;
            }
            else{
              tl.next = p;
            }
            tl = p;
          }
          //TreeBin代表红黑树，将TreeBin保存在哈希表的index位置
          setTabAt(tab,index,new TreeBin<K,V>(hd));
        }
      }
    }
  }
}

//tryPresize 方法用于哈希表的扩容
private final void tryPresize(int size){
  int c = (size >= (MAXIMUM_CAPACTITY >>> 1)) ? 
    MAXIMUM_CAPACTITY : tableSizeFor(size + (size >>> 1) +1);
  int sc;
  while((sc = sizeCtl) >= 0){
    Node<K,V>[] tab = table; int n;
    if(tab == null || (n=tab.length) == 0){
      n = (sc > c) ? sc : c;
      if(U.compareAndSwapInt(this,SIZECTL,sc,-1)){
        try{
          if(table == tab){
            Node<K,V>[] nt = (Node<K,V>[]) new Node<?,?>[n];
            table = nt;
            sc = n - (n >>> 2);
          }
        }finally{
          sizeCtl = sc;
        }
      }
    }
    else if(c <= sc || n >= MAXIMUM_CAPACTITY){
      break;
    }
    else if(tab == table){
      int rs = resizeStamp(n);
      if(sc < 0){
        Node<K,V>[] nt;
        if((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS 
          || (nt = nextTable) == null || transferIndex <= 0){
          break;
        }
        if(U.compareAndSwapInt(this,SIZECTL,sc,sc+1)){
          transfer(tab,nt);
        }
      }
      else if(U.compareAndSwapInt(this,SIZECTL,sc,(rs << RESIZE_STAMP_SHIFT) + 2)){
        transfer(tab,null);
      }
    }
  }
}
```

> concurrentHashMap的扩容时机和HashMap相同，都是在put方法的最后一步检查是否需要扩容，如果需要则进行扩容，但两者的扩容过程完全不同，concurrentHashMap的扩容方法叫做transfer，从put方法的addCount进入，transfer的主要思路是：
>
> 1、首先需要把老数组的值全部拷贝到扩容之后的新数组上，先从数组队尾开始拷贝
>
> 2、拷贝数组队槽点时，先把原数组槽点锁住，保证原数组槽点不能操作，成功拷贝到新数组时，把原数组槽点赋值为转移节点
>
> 3、这时如果有新数据正好put到此槽点，发现槽点为转移节点，就会一直等待，所以在扩容完成之前，该槽点对应的数据是不会发生改变的
>
> 4、从数组的尾部拷贝到头部，每拷贝成功一次，就把原数组中的节点设置为转移节点
>
> 5、直到所有数组数据都拷贝到新数组时，直接把新数组整个赋值给数组容器，拷贝完成
>
> 扩容方法主要是通过原数组上设置转移节点，put时碰到转移节点会等待扩容成功之后才put到策略，来保证整个扩容过程中肯定是线程安全的，因此数组的槽点一旦被设置为转移节点，在没有扩容完成之前是无法操作的

### 5、构造函数源码

```java
public ConcurrentHashMap(int initialCapacity){
  if(initialCapacity < 0){
    throw new IllegalArgumentException();
  }
  //如果传入的初始化容量值超过最大容量的一半，那么sizeCtl会被设置为最大容量
  //否则通过tableSizeFor方法就算出一个2的n次方数值作为size
  int cap = ((initialCapacity >= (MAXIMUM_CAPACITY >>> 1)) ?
             MAXIMUM_CAPACITY : tableSizeFor(initialCapacity+ (initialCapacity >>> 1) + 1);
  this.sizeCtl = cap;           
}
 
//二进制按位操作，最后得到的数值就是大与c的最小的2的n次方             
private static final int tableSizeFor(int c){
  int n  = c - 1;
  n |= n >>> 1;
  n |= n >>> 2;
  n |= n >>> 4;
  n |= n >>> 8;
  n |= n >>> 16;
  return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ?  MAXIMUM_CAPACITY : n+1;
}
             1、int n = 9 - 1n=82、n |= n >>> 1n=1000n >>> 1=0100两个值按位或后n=11003、n |= n >>> 2n=1100n >>> 2=0011n=1111复制代码

//推演一下过程
如果c是9；
int n = 9 -1 ;//8
n |= n >>> 1; //n=1000  n >>> 1=0100  两个值按位或后n=1100 
n |= n >>> 2; //n=1100  n >>> 2=0011  两个值按位或后n=1111
...
规律就是如果c足够大，使得n很大，那么运算到 n |= n >>> 16时，n的32位都是1
总结：这一段逻辑是把n有数值的bit位全部变为1，这样就得到了一个肯定大于等于n的值，最后一行代码的返回值最终返回的是n+1，那么一个所有位上都是1的二进制数字，+1后得到的就是一个2的n次方数值
```

> 有参数的构造函数：如果对于未来存储的数据量有预估，可以指定hash表的大小，tableSizeFor这个方法确保来哈希表的大小永远是2的n次方
>
> 这里传入的参数不是 initialCapacity，而是 initialCapacity 的 1.5 倍 + 1。这样做是为了保证在默认 75% 的负载因子下，能够足够容纳 initialCapacity 数量的元素。
>
> 总结：
>
> 1、构造函数中并不会初始化哈希表
>
> 2、构造函数中只设置哈希表大小的变量sizeCtl
>
> 3、initialCapacity并不是哈希表的大小
>
> 4、哈希表的大小为initialCapacity*1.5+1后，向上取最小的2的n次方，如果超过最大容量的一半，那么就是最大容量



